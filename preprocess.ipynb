{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_01_1 = pd.read_csv('kubu_01.csv')\n",
    "data_01_2 = pd.read_csv('dataset/#01.csv')\n",
    "data_01_3 = pd.read_csv('dataset/anies.csv')\n",
    "data_01_4 = pd.read_csv('dataset/cak_imin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_02_1 = pd.read_csv('kubu_02.csv')\n",
    "# data_02_2 = pd.read_csv('dataset/#02.csv')\n",
    "# data_02_3 = pd.read_csv('dataset/prabowo.csv')\n",
    "# data_02_4 = pd.read_csv('dataset/gibran.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_03_1 = pd.read_csv('kubu_03.csv')\n",
    "data_03_2 = pd.read_csv('dataset/#03.csv')\n",
    "data_03_3 = pd.read_csv('dataset/ganjar.csv')\n",
    "data_03_4 = pd.read_csv('dataset/mahfud.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01 = pd.concat([data_01_1, data_01_2, data_01_3, data_01_4], ignore_index=True)\n",
    "# merged_kubu_02 = pd.concat([data_02_1, data_02_2, data_02_3], ignore_index=True)\n",
    "merged_kubu_03 = pd.concat([data_03_1, data_03_2, data_03_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unnecessary Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['created_at', 'id_str', 'full_text', 'lang', 'location',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01 = merged_kubu_01[column_names]\n",
    "# merged_kubu_02 = merged_kubu_02[column_names]\n",
    "merged_kubu_03 = merged_kubu_03[column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.to_csv('all_kubu_01.csv', index=False)\n",
    "# merged_kubu_02.to_csv('all_kubu_02.csv', index=False)\n",
    "merged_kubu_03.to_csv('all_kubu_03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Column 'created_at' to Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_and_sort_created_at(df):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y').dt.date\n",
    "    return df.sort_values(by='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01 = simplify_and_sort_created_at(merged_kubu_01)\n",
    "# merged_kubu_02 = simplify_and_sort_created_at(merged_kubu_02)\n",
    "merged_kubu_03 = simplify_and_sort_created_at(merged_kubu_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.dropna(inplace=True)\n",
    "merged_kubu_01.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.dropna(inplace=True)\n",
    "# merged_kubu_02.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.dropna(inplace=True)\n",
    "merged_kubu_03.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.duplicated(subset=['id_str', 'full_text']).sum()\n",
    "# merged_kubu_02.duplicated(subset=['id_str', 'full_text']).sum()\n",
    "merged_kubu_03.duplicated(subset=['id_str', 'full_text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.drop_duplicates(subset=['id_str', 'full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.drop_duplicates(subset=['id_str', 'full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.drop_duplicates(subset=['id_str', 'full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Tweet That Aren't ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_kubu_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(merged_kubu_02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_kubu_03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01 = merged_kubu_01[merged_kubu_01['lang'] == 'in']\n",
    "# merged_kubu_02 = merged_kubu_02[merged_kubu_02['lang'] == 'in']\n",
    "merged_kubu_03 = merged_kubu_03[merged_kubu_03['lang'] == 'in']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop @Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].str.replace(r'@(?!\\w*(anies|anis)\\w*)\\w+', '', regex=True).str.strip()\n",
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].str.replace(r'@\\w*(anies|anis)\\w*', 'anies',  regex=True ).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].str.replace(r'@(?!\\w*(cakimin)\\w*)\\w+', '', regex=True).str.strip()\n",
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].str.replace(r'@\\w*(cakimin)\\w*', 'cak imin',  regex=True ).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].str.replace(r'@(?!\\w*(prabowo)\\w*)\\w+', '', regex=True).str.strip()\n",
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].str.replace(r'@\\w*(prabowo)\\w*', 'prabowo',  regex=True ).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].str.replace(r'@(?!\\w*(gibran)\\w*)\\w+', '', regex=True).str.strip()\n",
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].str.replace(r'@\\w*(gibran)\\w*', 'gibran',  regex=True ).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].str.replace(r'@(?!\\w*(ganjar)\\w*)\\w+', '', regex=True).str.strip()\n",
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].str.replace(r'@\\w*(ganjar)\\w*', 'ganjar',  regex=True ).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].str.replace(r'@(?!\\w*(mahfud)\\w*)\\w+', '', regex=True).str.strip()\n",
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].str.replace(r'@\\w*(mahfud)\\w*', 'mahfud md',  regex=True ).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Link in Colum Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return re.sub(r'https?://\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_pattern = r'\\banies\\b|\\banis\\b|\\b01\\b|\\bcak imin\\b|\\bimin\\b'\n",
    "date_pattern = r'\\b01/\\d{2}/\\d{4}\\b'\n",
    "number_pattern = r'\\d*01\\d*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_mask = merged_kubu_01['full_text'].str.contains(keyword_pattern, case=False, na=False)\n",
    "date_mask = merged_kubu_01['full_text'].str.contains(date_pattern, na=False)\n",
    "number_mask = merged_kubu_01['full_text'].str.contains(number_pattern, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask = keyword_mask | ~(date_mask | number_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01 = merged_kubu_01[final_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_kubu_01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_pattern = r'\\bprabowo\\b|\\bgibran\\b|\\b02\\b'\n",
    "# date_pattern = r'\\b02/\\d{2}/\\d{4}\\b'\n",
    "# number_pattern = r'\\d*02\\d*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_mask = merged_kubu_02['full_text'].str.contains(keyword_pattern, case=False, na=False)\n",
    "# date_mask = merged_kubu_02['full_text'].str.contains(date_pattern, na=False)\n",
    "# number_mask = merged_kubu_02['full_text'].str.contains(number_pattern, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_mask = keyword_mask | ~(date_mask | number_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02 = merged_kubu_02[final_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(merged_kubu_02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_pattern = r'\\bganjar\\b|\\bmahfud\\b|\\b03\\b'\n",
    "date_pattern = r'\\b03/\\d{2}/\\d{4}\\b'\n",
    "number_pattern = r'\\d*03\\d*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_mask = merged_kubu_03['full_text'].str.contains(keyword_pattern, case=False, na=False)\n",
    "date_mask = merged_kubu_03['full_text'].str.contains(date_pattern, na=False)\n",
    "number_mask = merged_kubu_03['full_text'].str.contains(number_pattern, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask = keyword_mask | ~(date_mask | number_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03 = merged_kubu_03[final_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_kubu_03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace &amp to &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].str.replace(r'&amp', '&', regex=False)\n",
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].str.replace(r'&amp', '&', regex=False)\n",
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].str.replace(r'&amp', '&', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_comments(text):\n",
    "    # Remove emojis by filtering out any character in emoji.EMOJI_DATA\n",
    "    emojis = [c for c in text if c not in emoji.EMOJI_DATA]\n",
    "\n",
    "    # Buat text menjadi lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "\n",
    "    # Bersihkan teks dari karakter khusus\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Bersihkan karakter yang berulang\n",
    "    normal_regex = re.compile(r\"(.)\\1{1,}\") # compiling regex pattern for a repeating character in a word (e.g., haiiii -> 'i' is repeated several times)\n",
    "    text = normal_regex.sub(r\"\\1\\1\", text) # removing the repeating characters \n",
    "    \n",
    "    # Hapus multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "       \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].apply(normalize_comments)\n",
    "merged_kubu_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].apply(normalize_comments)\n",
    "# merged_kubu_02.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].apply(normalize_comments)\n",
    "merged_kubu_03.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dict_from_json_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        dictionary = json.load(file)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = load_dict_from_json_file('combined_slang_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_slang_words(text, slang_words_dict=my_dict):\n",
    "    words = text.split()\n",
    "    standarized_words = []\n",
    "    for word in words:\n",
    "        standarized_word = slang_words_dict.get(word, word) # Mengembalikan kata asli jika kata tidak ditemukan dalam dict \n",
    "        standarized_words.append(standarized_word)\n",
    "    return ' '.join(standarized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].apply(normalize_slang_words)\n",
    "merged_kubu_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].apply(normalize_slang_words)\n",
    "# merged_kubu_02.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].apply(normalize_slang_words)\n",
    "merged_kubu_03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords_collection = pd.read_csv('new_kamusalay.csv', header=None, encoding='latin-1')\n",
    "replacements = dict(zip(slangwords_collection[0], slangwords_collection[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].apply(\n",
    "    lambda text: normalize_slang_words(text, replacements))\n",
    "merged_kubu_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].apply(\n",
    "#     lambda text: normalize_slang_words(text, replacements))\n",
    "# merged_kubu_02.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].apply(\n",
    "    lambda text: normalize_slang_words(text, replacements))\n",
    "merged_kubu_03.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  \n",
    "    \n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  \n",
    "    text = remove_whitespace(text)\n",
    "    stop_words = set(stopwords.words('indonesian')) \n",
    "    stop_words.update([])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].apply(remove_stopwords)\n",
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].apply(remove_stopwords)\n",
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed CSV for Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.to_csv('processed_merged_kubu_01.csv',index=False)\n",
    "# merged_kubu_02.to_csv('processed_merged_kubu_02.csv',index=False, encoding='utf-8-sig')\n",
    "merged_kubu_03.to_csv('processed_merged_kubu_03.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  \n",
    "    \n",
    "    token = nltk.word_tokenize(text)\n",
    "    stem_kalimat = []\n",
    "    for k in token:\n",
    "        stem_kata = stemmer.stem(k)\n",
    "        stem_kalimat.append(stem_kata)\n",
    "\n",
    "    stem_kalimat_str = ' '.join(stem_kalimat)\n",
    "    return stem_kalimat_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01['full_text'] = merged_kubu_01['full_text'].progress_apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['full_text'] = merged_kubu_02['full_text'].progress_apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['full_text'] = merged_kubu_03['full_text'].progress_apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_01.dropna(subset=['full_text'], inplace=True)\n",
    "token_data = [row.split() for row in merged_kubu_01['full_text']]\n",
    "all_words_no_stopwords = ' '.join([' '.join(tokens) for tokens in token_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words_no_stopwords)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hilangkan axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Pecah string panjang menjadi list of words (tokenisasi)\n",
    "token_list = all_words_no_stopwords.split()\n",
    "\n",
    "# Hitung frekuensi kata menggunakan Counter\n",
    "word_counts = Counter(token_list)\n",
    "\n",
    "# Ambil 20 kata yang paling sering muncul\n",
    "most_common_words = word_counts.most_common(20)\n",
    "\n",
    "# Pisahkan kata dan frekuensinya untuk plotting\n",
    "words, frequencies = zip(*most_common_words)\n",
    "\n",
    "# Plot Bar Chart untuk kata-kata paling sering\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(words, frequencies, color='lightgreen')\n",
    "\n",
    "# Menambahkan label frekuensi di dalam batang\n",
    "for bar, frequency in zip(bars, frequencies):\n",
    "    plt.text(bar.get_width() - 100,  # Mengatur agar teks berada sedikit di dalam batang\n",
    "             bar.get_y() + bar.get_height() / 2,  # Posisi vertikal\n",
    "             f'{frequency}',  # Nilai frekuensi yang akan ditampilkan\n",
    "             va='center', ha='right', color='black', fontsize=10)  # Posisi dan gaya teks\n",
    "\n",
    "# Label sumbu\n",
    "plt.xlabel('Frekuensi')\n",
    "plt.ylabel('Kata')\n",
    "plt.title('Top 20 Kata Terbanyak')\n",
    "\n",
    "# Membalik sumbu y agar kata dengan frekuensi tertinggi di atas\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Tampilkan plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "merged_kubu_01['created_at'] = pd.to_datetime(merged_kubu_01['created_at'])\n",
    "tweets_per_day = merged_kubu_01.groupby(merged_kubu_01['created_at'].dt.date).size()\n",
    "tweets_per_month = merged_kubu_01.groupby(merged_kubu_01['created_at'].dt.to_period('M')).size()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 20))\n",
    "\n",
    "# Per Day\n",
    "sns.lineplot(ax=axes[0], x=tweets_per_day.index, y=tweets_per_day.values, marker='o', linewidth=2)\n",
    "axes[0].set_title('Number of Tweets Per Day', fontsize=14)\n",
    "axes[0].set_xlabel('Date', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Tweets', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Per Month\n",
    "sns.barplot(ax=axes[1], x=tweets_per_month.index.astype(str), y=tweets_per_month.values, palette=\"Blues_r\")\n",
    "axes[1].set_title('Number of Tweets Per Month', fontsize=14)\n",
    "axes[1].set_xlabel('Month', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Tweets', fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02.dropna(subset=['full_text'], inplace=True)\n",
    "# token_data = [row.split() for row in merged_kubu_02['full_text']]\n",
    "# all_words_no_stopwords = ' '.join([' '.join(tokens) for tokens in token_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words_no_stopwords)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')  # Hilangkan axis\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pecah string panjang menjadi list of words (tokenisasi)\n",
    "# token_list = all_words_no_stopwords.split()\n",
    "\n",
    "# # Hitung frekuensi kata menggunakan Counter\n",
    "# word_counts = Counter(token_list)\n",
    "\n",
    "# # Ambil 20 kata yang paling sering muncul\n",
    "# most_common_words = word_counts.most_common(20)\n",
    "\n",
    "# # Pisahkan kata dan frekuensinya untuk plotting\n",
    "# words, frequencies = zip(*most_common_words)\n",
    "\n",
    "# # Plot Bar Chart untuk kata-kata paling sering\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bars = plt.barh(words, frequencies, color='lightgreen')\n",
    "\n",
    "# # Menambahkan label frekuensi di dalam batang\n",
    "# for bar, frequency in zip(bars, frequencies):\n",
    "#     plt.text(bar.get_width() - 100,  # Mengatur agar teks berada sedikit di dalam batang\n",
    "#              bar.get_y() + bar.get_height() / 2,  # Posisi vertikal\n",
    "#              f'{frequency}',  # Nilai frekuensi yang akan ditampilkan\n",
    "#              va='center', ha='right', color='black', fontsize=10)  # Posisi dan gaya teks\n",
    "\n",
    "# # Label sumbu\n",
    "# plt.xlabel('Frekuensi')\n",
    "# plt.ylabel('Kata')\n",
    "# plt.title('Top 20 Kata Terbanyak')\n",
    "\n",
    "# # Membalik sumbu y agar kata dengan frekuensi tertinggi di atas\n",
    "# plt.gca().invert_yaxis()\n",
    "\n",
    "# # Tampilkan plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_kubu_02['created_at'] = pd.to_datetime(merged_kubu_02['created_at'])\n",
    "# tweets_per_day = merged_kubu_02.groupby(merged_kubu_02['created_at'].dt.date).size()\n",
    "# tweets_per_month = merged_kubu_02.groupby(merged_kubu_02['created_at'].dt.to_period('M')).size()\n",
    "\n",
    "# fig, axes = plt.subplots(2, 1, figsize=(12, 20))\n",
    "\n",
    "# # Per Day\n",
    "# sns.lineplot(ax=axes[0], x=tweets_per_day.index, y=tweets_per_day.values, marker='o', linewidth=2)\n",
    "# axes[0].set_title('Number of Tweets Per Day', fontsize=14)\n",
    "# axes[0].set_xlabel('Date', fontsize=12)\n",
    "# axes[0].set_ylabel('Number of Tweets', fontsize=12)\n",
    "# axes[0].tick_params(axis='x', rotation=45)\n",
    "# axes[0].grid(True)\n",
    "\n",
    "# # Per Month\n",
    "# sns.barplot(ax=axes[1], x=tweets_per_month.index.astype(str), y=tweets_per_month.values, palette=\"Blues_r\")\n",
    "# axes[1].set_title('Number of Tweets Per Month', fontsize=14)\n",
    "# axes[1].set_xlabel('Month', fontsize=12)\n",
    "# axes[1].set_ylabel('Number of Tweets', fontsize=12)\n",
    "# axes[1].tick_params(axis='x', rotation=45)\n",
    "# axes[1].grid(axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03.dropna(subset=['full_text'], inplace=True)\n",
    "token_data = [row.split() for row in merged_kubu_03['full_text']]\n",
    "all_words_no_stopwords = ' '.join([' '.join(tokens) for tokens in token_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words_no_stopwords)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hilangkan axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pecah string panjang menjadi list of words (tokenisasi)\n",
    "token_list = all_words_no_stopwords.split()\n",
    "\n",
    "# Hitung frekuensi kata menggunakan Counter\n",
    "word_counts = Counter(token_list)\n",
    "\n",
    "# Ambil 20 kata yang paling sering muncul\n",
    "most_common_words = word_counts.most_common(20)\n",
    "\n",
    "# Pisahkan kata dan frekuensinya untuk plotting\n",
    "words, frequencies = zip(*most_common_words)\n",
    "\n",
    "# Plot Bar Chart untuk kata-kata paling sering\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(words, frequencies, color='lightgreen')\n",
    "\n",
    "# Menambahkan label frekuensi di dalam batang\n",
    "for bar, frequency in zip(bars, frequencies):\n",
    "    plt.text(bar.get_width() - 100,  # Mengatur agar teks berada sedikit di dalam batang\n",
    "             bar.get_y() + bar.get_height() / 2,  # Posisi vertikal\n",
    "             f'{frequency}',  # Nilai frekuensi yang akan ditampilkan\n",
    "             va='center', ha='right', color='black', fontsize=10)  # Posisi dan gaya teks\n",
    "\n",
    "# Label sumbu\n",
    "plt.xlabel('Frekuensi')\n",
    "plt.ylabel('Kata')\n",
    "plt.title('Top 20 Kata Terbanyak')\n",
    "\n",
    "# Membalik sumbu y agar kata dengan frekuensi tertinggi di atas\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Tampilkan plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_kubu_03['created_at'] = pd.to_datetime(merged_kubu_03['created_at'])\n",
    "tweets_per_day = merged_kubu_03.groupby(merged_kubu_03['created_at'].dt.date).size()\n",
    "tweets_per_month = merged_kubu_03.groupby(merged_kubu_03['created_at'].dt.to_period('M')).size()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 20))\n",
    "\n",
    "# Per Day\n",
    "sns.lineplot(ax=axes[0], x=tweets_per_day.index, y=tweets_per_day.values, marker='o', linewidth=2)\n",
    "axes[0].set_title('Number of Tweets Per Day', fontsize=14)\n",
    "axes[0].set_xlabel('Date', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Tweets', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Per Month\n",
    "sns.barplot(ax=axes[1], x=tweets_per_month.index.astype(str), y=tweets_per_month.values, palette=\"Blues_r\")\n",
    "axes[1].set_title('Number of Tweets Per Month', fontsize=14)\n",
    "axes[1].set_xlabel('Month', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Tweets', fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
